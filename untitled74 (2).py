# -*- coding: utf-8 -*-
"""Untitled74.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i1x067WebLkU8NtXnQbVl3asNtpyJJ80
"""

import os
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# -------------------------
# User config - EDIT THESE
# -------------------------
CSV_PATH = "data.csv"               # Path to your CSV file
DATETIME_COL = None                 # e.g. 'timestamp' or None if no datetime column
FEATURE_COLS = None                 # list of feature column names or None to use all except target
TARGET_COL = None                   # name of target column (single target)
LOOKBACK = 48                       # number of past time steps used as input
HORIZON = 6                         # forecast horizon (how many steps ahead to predict)
TEST_SIZE = 0.15
VAL_SIZE = 0.15
RANDOM_SEED = 42
BATCH_SIZE = 64
EPOCHS = 60
LR = 0.001
LSTM_UNITS = 64

# If you don't edit FEATURE_COLS/TARGET_COL the script will attempt to infer them.
# -------------------------

# -------------------------
# Utility functions
# -------------------------
def load_data(path):
    df = pd.read_csv(path)
    if DATETIME_COL and DATETIME_COL in df.columns:
        df[DATETIME_COL] = pd.to_datetime(df[DATETIME_COL])
        df = df.sort_values(DATETIME_COL).reset_index(drop=True)
    return df

def infer_columns(df):
    global FEATURE_COLS, TARGET_COL
    if TARGET_COL is None:
        # assume last column is target
        TARGET_COL = df.columns[-1]
    if FEATURE_COLS is None:
        FEATURE_COLS = [c for c in df.columns if c != DATETIME_COL and c != TARGET_COL]
    return FEATURE_COLS, TARGET_COL

def create_sequences(values, lookback, horizon):
    """Create sliding windows for multi-step forecasting.
    values: (T, D) numpy array
    returns: X shape (num_windows, lookback, D), y shape (num_windows, horizon)
    """
    X, y = [], []
    T = values.shape[0]
    for i in range(T - lookback - horizon + 1):
        X.append(values[i:i+lookback])
        y.append(values[i+lookback:i+lookback+horizon, 0])  # assume target is first column of values_target
    return np.array(X), np.array(y)

# -------------------------
# Attention layer (Bahdanau-style)
# -------------------------
class BahdanauAttention(layers.Layer):
    def _init_(self, units, **kwargs):
        super()._init_(**kwargs)
        self.W1 = layers.Dense(units)
        self.W2 = layers.Dense(units)
        self.V = layers.Dense(1)

    def call(self, encoder_outputs, decoder_state=None):
        # encoder_outputs: (batch, time, features)
        # decoder_state: optional (batch, units) - we can broadcast it to compare
        if decoder_state is None:
            # use zero vector to compute self-attention across encoder outputs
            decoder_state = tf.zeros((tf.shape(encoder_outputs)[0], self.W1.units))
        # expand decoder_state to time axis
        decoder_with_time = tf.expand_dims(decoder_state, 1)  # (batch, 1, units)
        score = self.V(tf.nn.tanh(self.W1(encoder_outputs) + self.W2(decoder_with_time)))  # (batch, time, 1)
        attention_weights = tf.nn.softmax(score, axis=1)  # (batch, time, 1)
        context_vector = attention_weights * encoder_outputs  # (batch, time, features)
        context_vector = tf.reduce_sum(context_vector, axis=1)  # (batch, features)
        # return context vector and weights for visualization
        return context_vector, tf.squeeze(attention_weights, axis=-1)  # weights shape (batch, time)

# -------------------------
# Model definitions
# -------------------------
def build_lstm_attention(input_shape, lstm_units=LSTM_UNITS, horizon=HORIZON):
    """
    Encoder LSTM -> Attention -> Dense outputs for multi-step forecast
    input_shape: (lookback, features)
    """
    inputs = Input(shape=input_shape)  # (lookback, features)
    x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(inputs)  # (batch, time, 2*lstm_units)
    # use last hidden (mean) as 'decoder' state input to attention (simple)
    pooled = layers.GlobalAveragePooling1D()(x)  # (batch, 2*lstm_units)
    attention = BahdanauAttention(units=128)
    context, att_weights = attention(x, pooled)  # context (batch, features)
    # optionally combine context and pooled
    combined = layers.Concatenate()([context, pooled])
    dense = layers.Dense(128, activation='relu')(combined)
    outputs = layers.Dense(horizon, name='forecast')(dense)  # predict horizon steps ahead (parallel)
    model = Model(inputs=inputs, outputs=[outputs, att_weights])
    model.compile(optimizer=Adam(learning_rate=LR), loss='mse')
    return model

def build_baseline_lstm(input_shape, lstm_units=LSTM_UNITS, horizon=HORIZON):
    inputs = Input(shape=input_shape)
    x = layers.LSTM(lstm_units, return_sequences=False)(inputs)
    x = layers.Dense(128, activation='relu')(x)
    outputs = layers.Dense(horizon)(x)
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer=Adam(learning_rate=LR), loss='mse')
    return model

# -------------------------
# Metrics
# -------------------------
def rmse(a, b): return np.sqrt(mean_squared_error(a, b))
def mae(a, b): return mean_absolute_error(a, b)
def mape(a, b): return np.mean(np.abs((a - b) / (a + 1e-8))) * 100

# -------------------------
# Main pipeline
# -------------------------
def main():
    # Load
    if not os.path.exists(CSV_PATH):
        raise FileNotFoundError(f"CSV not found: {CSV_PATH}\nPlace your CSV at this path or update CSV_PATH.")
    df = load_data(CSV_PATH)
    features, target = infer_columns(df)
    print("Using features:", features)
    print("Target:", target)

    # Prepare arrays: We'll scale features and the target together for convenience
    arr_features = df[features].values.astype(float)
    arr_target = df[[target]].values.astype(float)  # keep shape (T,1)

    # Concat target as first column of model inputs for create_sequences convenience
    combined = np.concatenate([arr_target, arr_features], axis=1)  # shape (T, 1+F)

    # Scale
    scaler = StandardScaler()
    combined_scaled = scaler.fit_transform(combined)

    # Create sequences
    X, y = create_sequences(combined_scaled, LOOKBACK, HORIZON)
    # Note: y is currently scaled target values (since target is first col)
    print("Created sequences:", X.shape, y.shape)

    # Train / Validation / Test split (time-aware - keep order)
    total = X.shape[0]
    test_n = int(total * TEST_SIZE)
    val_n = int(total * VAL_SIZE)
    train_n = total - val_n - test_n

    X_train = X[:train_n]; y_train = y[:train_n]
    X_val = X[train_n:train_n+val_n]; y_val = y[train_n:train_n+val_n]
    X_test = X[train_n+val_n:]; y_test = y[train_n+val_n:]

    print("Splits:", X_train.shape, X_val.shape, X_test.shape)

    input_shape = X_train.shape[1:]  # (lookback, features_total)
    # Build models
    att_model = build_lstm_attention(input_shape)
    baseline = build_baseline_lstm(input_shape)

    # Training attention model (it returns two outputs, but we train on first - forecast)
    # Keras expects y shape matching model.outputs[0]
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4)
    ]

    print("Training attention model...")
    history_att = att_model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=EPOCHS, batch_size=BATCH_SIZE,
        callbacks=callbacks, verbose=2
    )

    # Baseline training
    print("Training baseline LSTM...")
    history_base = baseline.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=EPOCHS, batch_size=BATCH_SIZE,
        callbacks=callbacks, verbose=2
    )

    # Predictions and evaluation
    # For attention model Keras returns [pred, attention_weights] if called predict()
    pred_att, att_weights = att_model.predict(X_test, batch_size=BATCH_SIZE)
    pred_base = baseline.predict(X_test, batch_size=BATCH_SIZE)

    # Inverse scale predictions for metrics: we scaled combined columns together.
    # To inverse transform we need to place predictions into arrays shaped like combined columns
    def inv_scale_preds(pred_scaled, scaler, reference_column_index=0):
        # pred_scaled: (n_samples, horizon) scaled values for target
        # We will create a placeholder array of shape (n_samples, n_columns) repeated across horizon steps
        n_samples, horizon = pred_scaled.shape
        n_cols = combined.shape[1]
        inv = []
        for t in range(horizon):
            # for each forecast step create sample rows with target=pred_scaled[:,t] and other features zeros
            placeholder = np.zeros((n_samples, n_cols))
            placeholder[:, 0] = pred_scaled[:, t]  # target is first column
            inv_t = scaler.inverse_transform(placeholder)[:, 0]  # get back target column only
            inv.append(inv_t)
        # return shape (n_samples, horizon)
        return np.stack(inv, axis=1)

    y_test_inv = inv_scale_preds(y_test, scaler)
    pred_att_inv = inv_scale_preds(pred_att, scaler)
    pred_base_inv = inv_scale_preds(pred_base, scaler)

    # Evaluate on whole horizon by averaging metrics across all horizon steps
    def evaluate(y_true, y_pred, name="model"):
        rm = rmse(y_true.flatten(), y_pred.flatten())
        ma = mae(y_true.flatten(), y_pred.flatten())
        mp = mape(y_true.flatten(), y_pred.flatten())
        print(f"{name} -> RMSE: {rm:.4f}, MAE: {ma:.4f}, MAPE: {mp:.2f}%")
        return rm, ma, mp

    print("\nEvaluation on test set:")
    evaluate(y_test_inv, pred_att_inv, "Attention-LSTM")
    evaluate(y_test_inv, pred_base_inv, "Baseline-LSTM")

    # Visualize predictions for the first few test samples and attention weights
    n_plot = 4
    t_idx = 0
    for i in range(n_plot):
        plt.figure(figsize=(9,3))
        plt.plot(range(HORIZON), y_test_inv[i], label='true')
        plt.plot(range(HORIZON), pred_att_inv[i], label='att_pred')
        plt.plot(range(HORIZON), pred_base_inv[i], label='base_pred')
        plt.title(f"Sample {i} - Forecast horizon")
        plt.legend()
        plt.show()

    # Attention weights shape: (n_test, lookback)
    print("Attention weights shape:", att_weights.shape)
    # show attention for a few samples
    for i in range(min(3, att_weights.shape[0])):
        plt.figure(figsize=(8,2.5))
        plt.imshow(att_weights[i:i+1], aspect='auto')
        plt.colorbar(label='attention weight')
        plt.title(f"Attention weights for test sample {i} (time axis=lookback steps)")
        plt.xlabel("time step (0=earliest in lookback)")
        plt.yticks([])
        plt.show()

    # Save models and scaler for reproducibility
    att_model.save("attention_lstm_model", include_optimizer=False)
    baseline.save("baseline_lstm_model", include_optimizer=False)
    import joblib
    joblib.dump(scaler, "scaler.save")
    print("Saved models and scaler.")